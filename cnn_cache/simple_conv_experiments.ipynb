{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_conv_experiments.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tavishcode/fyp/blob/master/cnn_cache/simple_conv_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnYc3sJBwswr",
        "colab_type": "code",
        "outputId": "dadabf76-0a92-42db-ccdd-3c52a51e2d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import (Input, Activation, Dense, Dropout, Conv1D,\n",
        "                          Lambda, Concatenate)\n",
        "from keras.callbacks import EarlyStopping\n",
        "import pymongo\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpgqbRiniXYa",
        "colab_type": "code",
        "outputId": "eba60338-f520-4b43-c802-7d0d8614910f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "!pip install sacred\n",
        "from sacred import Experiment\n",
        "from sacred.observers import MongoObserver"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacred\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/25/844c1e9bf5e767b76b9c1df9b7fc8ea323ea8c5065a6bcd9659a33d6db81/sacred-0.7.4-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.3MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=4.0 (from sacred)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/60/63f28a5401da733043abe7053e7d9591491b4784c4f87c339bf51215aa0a/py-cpuinfo-5.0.0.tar.gz (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt<1.0,>=0.3 in /usr/local/lib/python3.6/dist-packages (from sacred) (0.6.2)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from sacred) (1.10.11)\n",
            "Collecting munch<3.0,>=2.0.2 (from sacred)\n",
            "  Downloading https://files.pythonhosted.org/packages/68/f4/260ec98ea840757a0da09e0ed8135333d59b8dfebe9752a365b04857660a/munch-2.3.2.tar.gz\n",
            "Collecting jsonpickle<1.0,>=0.7.2 (from sacred)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/d5/2f47f03d3f64c31b0d7070b488274631d7567c36e81a9f744e6638bb0f0d/jsonpickle-0.9.6.tar.gz (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 30.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch<3.0,>=2.0.2->sacred) (1.12.0)\n",
            "Building wheels for collected packages: py-cpuinfo, munch, jsonpickle\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/7e/a9/b982d0fea22b7e4ae5619de949570cde5ad55420cec16e86a5\n",
            "  Building wheel for munch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/bf/bc/06a3e1bfe0ab27d2e720ceb3cff3159398d92644c0cec2c125\n",
            "  Building wheel for jsonpickle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/8b/41/8ce98f4737a9ff61b1bf2673f2abfe66a6a43ad6e91d2c9736\n",
            "Successfully built py-cpuinfo munch jsonpickle\n",
            "Installing collected packages: py-cpuinfo, munch, jsonpickle, sacred\n",
            "Successfully installed jsonpickle-0.9.6 munch-2.3.2 py-cpuinfo-5.0.0 sacred-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu6P7sM_yGKC",
        "colab_type": "code",
        "outputId": "54216e1d-c94a-471d-c7c7-0f7af94c214b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train = np.load('drive/My Drive/train_set3.npy')\n",
        "train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16384, 200, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3a8vFvFkc8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape_data(w_size, pred_length, portal, num_features):\n",
        "    num_features = 11 if portal else 1\n",
        "    encoder_input = []\n",
        "    decoder_output = []\n",
        "    for i in range(train.shape[0]): # for each content type i\n",
        "        if i % 1000 == 0:\n",
        "            clear_output(wait = True)\n",
        "            print(i, \"/\", train.shape[0]-1)\n",
        "        for j in range(w_size, train.shape[1]-pred_length+1): # for each timestep\n",
        "            encoder_input.append([]) # add encoder input sample\n",
        "            decoder_output.append([]) # add decoder input sample\n",
        "            for w in range(w_size): # add lagged features\n",
        "                encoder_input[-1].append(train[i, j-w_size+w, :num_features])\n",
        "            for p in range(pred_length): # add true labels for desired pred length\n",
        "                decoder_output[-1].append(train[i, j+p, :num_features])\n",
        "        encoder_input = np.asarray(encoder_input, dtype='float')\n",
        "        decoder_output = np.asarray(decoder_output, dtype='float')\n",
        "        clear_output(wait = True)\n",
        "        print(i, \"/\", train.shape[0]-1)\n",
        "    return encoder_input, decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD0KQTe9lika",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def slice(x, seq_length):\n",
        "    return x[:,-seq_length:,:]\n",
        "  \n",
        "def make_model(portal,\n",
        "                n_layer,\n",
        "                n_filter,\n",
        "                k_size,\n",
        "                hidden_unit,\n",
        "                dropout,\n",
        "                pred_length,\n",
        "                loss,\n",
        "                num_features):\n",
        "    dilation_rates = [2**i for i in range(n_layer)] \n",
        "    history_seq = Input(shape=(None, num_features))\n",
        "    x = history_seq\n",
        "    for dilation_rate in dilation_rates:\n",
        "        x = Conv1D(filters=n_filter, \n",
        "                 kernel_size=k_size, \n",
        "                 padding='causal', \n",
        "                 dilation_rate=dilation_rate)(x)\n",
        "    x = Dense(hidden_unit, activation='relu')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = Dense(num_features)(x)\n",
        "    pred_seq_train = Lambda(slice, arguments={'seq_length':pred_length})(x)\n",
        "    model = Model(history_seq, pred_seq_train)\n",
        "    model.compile(Adam(), loss=loss)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lScHtwpSr8jR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex = Experiment('simple_conv_experiments', interactive=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWW9T5rl6pV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = ('mongodb://tavish:1234@cluster0-shard-00-00-anyrv.mongodb.net:27017,'\n",
        "+'cluster0-shard-00-01-anyrv.mongodb.net:27017,'\n",
        "+'cluster0-shard-00-02-anyrv.mongodb.net:27017/'\n",
        "+'test?ssl=true&replicaSet=Cluster0-shard-0&authSource=admin&retryWrites=true')\n",
        "ex.observers.append(MongoObserver.create(url=url, db_name='test'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuugZ2v7uGdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@ex.main\n",
        "def run(n_layer,\n",
        "        n_filter,\n",
        "        k_size,\n",
        "        dropout,\n",
        "        hidden_unit,\n",
        "        loss,\n",
        "        b_size,\n",
        "        w_size,\n",
        "        pred_length,\n",
        "        portal,\n",
        "        epochs,\n",
        "        _run):\n",
        "    num_features = 11 if portal else 1\n",
        "    encoder_input, decoder_output = reshape_data(w_size, \n",
        "                                               pred_length, \n",
        "                                               portal,\n",
        "                                               num_features)\n",
        "    model = make_model( portal,\n",
        "                      n_layer,\n",
        "                      n_filter,\n",
        "                      k_size,\n",
        "                      hidden_unit,\n",
        "                      dropout,\n",
        "                      pred_length,\n",
        "                      loss,\n",
        "                      num_features)\n",
        "    lagged_target_history = decoder_output[:,:-1,:num_features]\n",
        "    encoder_input = np.concatenate([encoder_input, lagged_target_history], \n",
        "                                 axis=1)\n",
        "    hist = model.fit(encoder_input, decoder_output, batch_size=b_size, epochs=epochs)\n",
        "    run.log_scalar(\"training.loss\", hist.history['loss'][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kny6JunT3zsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convolutional layer parameters\n",
        "num_layers = [2**3, 2**4, 2**5]\n",
        "num_filters = [2**3, 2**4, 2**5] \n",
        "kernel_sizes = [2,4,7]\n",
        "losses = ['mean_absolute_error', 'mean_squared_error']\n",
        "batch_sizes = [2**3, 2**6, 2**8]\n",
        "portal_settings = [True, False]\n",
        "epoch_settings = [2,5,10]\n",
        "dropouts = [0, .2, .4, .6]\n",
        "hidden_units = [2**3, 2**5, 2**7]\n",
        "window_sizes = [7, 14, 28]\n",
        "pred_length = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqZRhFUTwvyw",
        "colab_type": "code",
        "outputId": "a2a80870-76da-4876-aae9-4b8d06b46d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('# of experiments:', \n",
        "      len(num_layers)\n",
        "      *len(num_filters)\n",
        "      *len(kernel_sizes)\n",
        "      *len(losses)\n",
        "      *len(batch_sizes)\n",
        "      *len(portal_settings)\n",
        "      *len(epoch_settings)\n",
        "      *len(dropouts)\n",
        "      *len(hidden_units)\n",
        "      *len(window_sizes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of experiments: 34992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-APrNf6_7ct",
        "colab_type": "code",
        "outputId": "f63e868f-4370-467e-e806-738c3eb4264f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for n_layer in num_layers:\n",
        "    for n_filter in num_filters:\n",
        "        for k_size in kernel_sizes:\n",
        "            for dropout in dropouts:\n",
        "                for hidden_unit in hidden_units:\n",
        "                    for loss in losses:\n",
        "                        for b_size in batch_sizes:\n",
        "                            for w_size in window_sizes:\n",
        "                                for portal_setting in portal_settings:\n",
        "                                    for epochs in epoch_settings:\n",
        "                                        r = ex.run(config_updates={\n",
        "                                          'n_layer': n_layer,\n",
        "                                          'n_filter': n_filter,\n",
        "                                          'k_size': k_size,\n",
        "                                          'dropout': dropout,\n",
        "                                          'hidden_unit': hidden_unit,\n",
        "                                          'loss': loss,\n",
        "                                          'b_size': b_size,\n",
        "                                          'w_size': w_size,\n",
        "                                          'pred_length': pred_length,\n",
        "                                          'portal': portal_setting,\n",
        "                                          'epochs': epochs,\n",
        "                                        })"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8191 / 8191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBmftC02prAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}